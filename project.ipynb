{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task at hand is to analyze the data of the top rated dresses at the famous online shop, Shein, our primary source of data is the global Shein website. The follwong link is for the dresses page with the sorting based on rating:  \n",
    "\n",
    "https://www.shein.com/Women-Dresses-c-1727.html?adp=28869201&categoryJump=true&ici=www_tab03navbar03menu01dir05&src_identifier=fc%3DWomen%20Clothing%60sc%3DWomen%20Clothing%60tc%3DShop%20by%20category%60oc%3DDresses%60ps%3Dtab03navbar03menu01dir05%60jc%3Dreal_1727&src_module=topcat&src_tab_page_id=page_home1710957580759&sort=7  \n",
    "\n",
    "the collection of data took place at 19/03/2024 in the span of approximately 5 hours.  \n",
    "\n",
    "Since Shein website is dynamic like most of the websites that made us resort to using Selenium, that is an open-source automated testing tool primarily used for automating web applications. in the context of python, we used Selenium webDriver library along with Microsoft Edge driver to simulate user interactions which bypass the problem of dynamic websites.\n",
    "\n",
    "scrabing dynamic websites introduce many challenges that may be lesser or doesn't exist in static websites, the challenges that we faced are:  \n",
    "1.  the scrabing process depend highly on the loading time of the website, that is if a specific element in the website loaded slower than the time constraint that are specified in the code, the process will fail resulting in a runtime error.  \n",
    "2. To guard against bots and potintial attackers, Shein has a CAPTHCA that may appear once in a while, so, a person must attend the process the whole time so that when a CAPTCHA appear that person can perform the verfication herself.  \n",
    "3. The time taken to complete the entire process was excessive.  \n",
    "4. We needed some time to familiarize ourselves with Selenium since it's a new tool for us.  \n",
    "\n",
    "The collecting process was devided into two parts:\n",
    "1. collecting the URLs in a file.  \n",
    "2. Scrabing each of the link.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "import time\n",
    "\n",
    "\n",
    "# uncomment if the libraries are not downloaded.\n",
    "\n",
    "# %pip install selenium\n",
    "# %pip pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links=[]\n",
    "totalLinks=0\n",
    "\n",
    "\n",
    "for i in range(1,14):\n",
    "    l=\"https://www.shein.com/Women-Dresses-c-1727.html?adp=28045547&categoryJump=true&ici=www_tab03navbar03menu01dir05&src_identifier=fc%3DWomen%20Clothing%60sc%3DWomen%20Clothing%60tc%3DShop%20by%20category%60oc%3DDresses%60ps%3Dtab03navbar03menu01dir05%60jc%3Dreal_1727&src_module=topcat&src_tab_page_id=page_home1710836812106&sort=7&page=\"+str(i)\n",
    "    links.append(l)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "service = Service(EdgeChromiumDriverManager().install())\n",
    "\n",
    "# Initialize the Edge WebDriver with the service\n",
    "driver = webdriver.Edge(service=service)\n",
    "driver.get(\"https://www.shein.com\")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "script = \"document.querySelector('#txt_lang').click()\"\n",
    "driver.execute_script(script)\n",
    "\n",
    "\n",
    "for url in links :\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "    elements = driver.find_elements(By.CSS_SELECTOR, '.goods-title-link')\n",
    "    \n",
    "        \n",
    "    \n",
    "    totalLinks+=len(elements)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    with open(\"links.txt\",\"a\") as file:\n",
    "        for link in elements:\n",
    "         file.write(link.get_attribute(\"href\")+\"\\n\")\n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "driver.quit()\n",
    "print(\"total: \",totalLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The links that we collected are included in the file links.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrabing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open(\"sheinDresses10.csv\",\"a\") as file:\n",
    "    file.write(\"Rating,Final Price,Discount,Number of Reviews,Material,Color,Style,Type,Brand\\n\")\n",
    "\n",
    "service = Service(EdgeChromiumDriverManager().install())\n",
    "\n",
    "# Initialize the Edge WebDriver with the service\n",
    "driver = webdriver.Edge(service=service)\n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.shein.com/SHEIN-Qutie-Polo-Collar-Drop-Shoulder-Letter-Graphic-Pullover-p-11505778-cat-1773.html?src_identifier=fc%3DWomen%20Clothing%60sc%3DWomen%20Clothing%60tc%3D0%60oc%3D0%60ps%3Dtab03navbar03%60jc%3DitemPicking_017172961&src_module=topcat&src_tab_page_id=page_goods_detail1708318659597&mallCode=1&imgRatio=3-4\")\n",
    "\n",
    "\n",
    "\n",
    "element = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#txt_lang')))\n",
    "               \n",
    "\n",
    "script = \"document.querySelector('#txt_lang').click()\"\n",
    "driver.execute_script(script) \n",
    "time.sleep(7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "urls=[]\n",
    "\n",
    "with open(\"links10.txt\",\"r\") as urls:\n",
    "\n",
    "    for i,url in enumerate(urls) :\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "        \n",
    "            driver.get(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "           \n",
    "\n",
    "            element = WebDriverWait(driver,40).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.name-line .title')))\n",
    "            element = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.rate-num')))\n",
    "            element = WebDriverWait(driver,20).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.product-intro__description-table-item')))\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "           \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            descriptions = driver.find_elements(By.CSS_SELECTOR, '.product-intro__description-table-item')\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            color=\"\" \n",
    "            discount=\"0\"\n",
    "            material=\"\"\n",
    "            type=\"\"\n",
    "            style=\"\"\n",
    "            brand=\"\"\n",
    "            globalRating=\"0\"\n",
    "            finalPrice=\"0\"\n",
    "            numOfRev=\"0\"\n",
    "\n",
    "            for line in descriptions:\n",
    "                try:\n",
    "                    key = line.find_element(By.CSS_SELECTOR, '.key').get_attribute(\"innerHTML\")\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                if \"Color\" in key:\n",
    "                    color=line.find_element(By.CSS_SELECTOR, '.val').get_attribute('innerText')\n",
    "                elif \"Material\" in key:\n",
    "                    material=line.find_element(By.CSS_SELECTOR, '.val').get_attribute('innerText')\n",
    "                elif (key==\"Type: \"):\n",
    "                    type=line.find_element(By.CSS_SELECTOR, '.val').get_attribute('innerText')\n",
    "                elif (key==\"Style: \"):\n",
    "                    style=line.find_element(By.CSS_SELECTOR, '.val').get_attribute('innerText')\n",
    "\n",
    "\n",
    "            if(color==\"\" or material==\"\" or type==\"\" or style==\"\"):\n",
    "                    continue\n",
    "                \n",
    "\n",
    "           \n",
    "\n",
    "            try:\n",
    "                finalPrice=driver.find_element(By.CSS_SELECTOR, '.original span').get_attribute('innerText')\n",
    "            \n",
    "            except:\n",
    "                finalPrice=driver.find_element(By.CSS_SELECTOR, '.discount.from span').get_attribute('innerText')\n",
    "                discount=driver.find_element(By.CSS_SELECTOR, '.discount-label').get_attribute('innerText')\n",
    "                    \n",
    "\n",
    "           \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "            globalRating=driver.find_element(By.CSS_SELECTOR, 'div.rate-num').get_attribute('innerText')\n",
    "            \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "            try:\n",
    "                brand = driver.find_element(By.CSS_SELECTOR, '.name-line .title').get_attribute('innerText')\n",
    "            except Exception as e:\n",
    "                \n",
    "                    brand = driver.find_element(By.CSS_SELECTOR, '.product-intro__brand-title').get_attribute('innerText')\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            numOfRev=driver.find_element(By.CSS_SELECTOR, \".product-intro__head-reviews-text\").get_attribute('innerText')\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            strings=[url,color,material,type,style,brand,numOfRev]\n",
    "            for i in range(len(strings)):\n",
    "                strings[i] = strings[i].replace(\",\", \"-\")\n",
    "\n",
    "\n",
    "\n",
    "            print(strings[0]+\"\\n\"+strings[1]+\"\\n\"+strings[2]+\"\\n\"+strings[3]+\"\\n\"+strings[4]+\"\\n\"+strings[5]+\"\\n\"+strings[6]+\"\\n\"+finalPrice+\"\\n\"+discount+\"\\n\"+str(globalRating)+\"\\n\")\n",
    "            print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "            with open(\"sheinDresses.csv\",\"a\") as file:\n",
    "                strings[0]=strings[0].replace('\\n', '')\n",
    "                \n",
    "                file.write(+str(globalRating)+\",\"+finalPrice+\",\"+discount+\",\"+strings[6]+\",\"+strings[2]+\",\"+strings[1]+\",\"+strings[4]+\",\"+strings[3]+\",\"+strings[5]+\"\\n\")\n",
    "               \n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to resources constraints and limited computational power, we made the decision to limit our dataset to 1200 rows. This decision allows us to gather sufficient data to derive meaningful insights while ensuring that our analysis remains manageable and doesn't strain our computer and takes too much time. This approach enables us to efficiently utilize available resources while still gaining valuable understanding from our data. The raw data that we collected are in the file SheinDresses.csv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
